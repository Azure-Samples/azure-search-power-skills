{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = pypdf.PdfReader('../pdf/modelcard.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages = len(pdf_reader.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "pages_text = []\n",
    "\n",
    "for i in range(n_pages):\n",
    "    pages_text.append({'page_number': i + 1, 'text': pdf_reader.pages[i].extract_text()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1,\n",
       "  'text': 'model_card.md 2024-10-07\\n1 / 5Phi-3.5-Mini-Instruct ONNX models\\nThis repository hosts the optimized versions of Phi-3.5-mini-instruct to accelerate inference with ONNX Runtime.\\nOptimized Phi-3.5 Mini models are published here in ONNX format to run with ONNX Runtime on CPU and GPU\\nacross devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the\\nprecision best suited to each of these targets.\\nTo easily get started with Phi-3.5, you can use our newly introduced ONNX Runtime Generate() API. See here for\\ninstructions on how to run it.\\nONNX Models\\nHere are some of the optimized configurations we have added:\\n\\x00. ONNX model for fp16 CUDA: ONNX model you can use to run for your NVIDIA GPUs.\\n\\x00. ONNX model for int4 CUDA: ONNX model for NVIDIA GPUs using int4 quantization via AWQ.\\n\\x00. ONNX model for int4 CPU and Mobile: ONNX model for CPU and mobile using int4 quantization via AWQ.\\nModel Summary\\nPhi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and\\nfiltered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to\\nthe Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement\\nprocess, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference\\noptimization to ensure precise instruction adherence and robust safety measures.\\nIntended Uses\\nThe Phi 3.5 model is intended for commercial and research use in multiple languages. The model provides uses for\\ngeneral purpose AI systems and applications which require:\\n\\x00. Memory/compute constrained environments\\n\\x00. Latency bound scenarios\\n\\x00. Strong reasoning (especially code, math and logic)\\nUse Case Considerations\\nPhi 3.5 models are not specifically designed or evaluated for all downstream purposes. Developers should\\nconsider common limitations of language models as they select use cases, and evaluate and mitigate for\\naccuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk\\nscenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade\\ncompliance laws, etc.) that are relevant to their use case. Nothing contained in this Model Card should be\\ninterpreted as or deemed a restriction or modification to the license the model is released under.\\nRelease Notes\\nThis is an update over the instruction-tuned Phi-3 Mini ONNX model release. We believe most use cases will\\nbenefit from this release, but we encourage users to test their particular AI applications. We appreciate the\\nenthusiastic adoption of the Phi-3 model family and continue to welcome all feedback from the community.\\nHardware Supported'},\n",
       " {'page_number': 2,\n",
       "  'text': 'model_card.md 2024-10-07\\n2 / 5The ONNX models are tested on:\\nGPU SKU: RTX 4090 (DirectML)\\nGPU SKU: 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4 (CUDA)\\nCPU SKU: Standard D16s v6 (16 vcpus, 64 GiB memory)\\nAMD CPU: Internal_D64as_v5\\nMinimum Configuration Required:\\nWindows: DirectX 12-capable GPU and a minimum of 4GB of combined RAM\\nCUDA: NVIDIA GPU with Compute Capability >= 7.0\\nModel Description\\nDeveloped by: Microsoft\\nModel type: ONNX\\nLanguage(s) (NLP): Python, C, C++\\nLicense: MIT\\nModel Description: This is a conversion of the Phi-3.5 Mini-Instruct model for ONNX Runtime inference.\\nHow to Get Started with the Model\\nTo make running of the Phi-3 models across a range of devices and platforms across various execution provider\\nbackends possible, we introduce a new API to wrap several aspects of generative AI inferencing. This API make it\\neasy to drag and drop LLMs straight into your app. For running the early version of these models with ONNX\\nRuntime, follow the steps here.\\nFor example:\\npython model-qa.py -m /*{YourModelPath}*/Phi -3.5-mini-instruct-\\nonnx/cpu_and_mobile/cpu-int4-awq-block -128-acc-level -4 -k 40 -p 0.95 -t 0.8 -r \\n1.0\\n*Input:*  <|user|>Tell me a joke<|end|><|assistant|>  \\n \\n*Output:*  Why don\\'t scientists trust atoms?  \\n           Because they make up everything!  \\n \\nThis joke plays on the double meaning of \"make up.\" In science, atoms are the  \\nfundamental building blocks of matter, literally making up everything. However,  \\nin a colloquial sense, \"to make up\" can mean to fabricate or lie, hence the  \\nhumor. \\nPerformance Metrics\\nPhi-3.5 Mini-Instruct performs better in ONNX Runtime than PyTorch for all batch size, prompt length\\ncombinations.\\nThe table below shows the average throughput of the first 256 tokens generated (tps) for FP16 and INT4\\nprecisions on CUDA as measured on 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4. ONNX Runtime'},\n",
       " {'page_number': 3,\n",
       "  'text': 'model_card.md 2024-10-07\\n3 / 5models for GPU are 21X faster than PyTorch Compile and up to 8X faster than llama.cpp on A100 GPU.\\nBatch\\nSize,\\nSequence\\nLengthONNX\\nRT\\nINT4PyTorch\\nEager\\nINT4PyTorch\\nCompile\\nINT4Llama.cpp\\nINT4INT4\\nSpeedUp\\nORT/PyTorch\\nEagerINT4\\nSpeedUp\\nORT/PyTorch\\nCompileINT4 SpeedUp\\nORT/Llama.cpp\\n1, 16238.9717.7511.36183.1713.4621.041.30\\n1, 64233.7417.7411.32182.7713.1720.651.28\\n1, 256208.5217.8211.34182.1511.7018.381.14\\n1, 1024174.1917.8511.36166.399.7615.341.05\\n1, 2048146.1017.9611.35153.508.1412.870.95\\n1, 3840112.6817.9111.34141.536.299.940.80\\n4, 16286.7360.9040.89180.824.717.011.59\\n4, 64282.8760.8841.03177.694.656.891.59\\n4, 256268.3060.8540.90166.344.416.561.61\\n4, 1024223.3060.8640.90133.393.675.461.67\\n4, 2048187.6260.8040.93106.033.094.581.77\\n4, 3840145.5955.9640.8878.122.603.561.86\\n8, 16541.04121.9281.96171.904.446.603.15\\n8, 64532.68121.8781.98166.334.376.503.20\\n8, 256480.00122.0681.80148.073.935.873.24\\n8, 1024360.60122.4881.59103.582.944.423.48\\n8, 2048274.16105.9281.7174.012.593.363.70\\n8, 3840192.5079.7481.5049.232.412.363.91\\n16, 161007.69244.16163.09156.994.136.186.42\\n16, 64966.42244.89163.26148.233.955.926.52\\n16, 256827.37244.84163.23121.853.385.076.79\\n16, 1024536.73209.13169.3071.572.573.177.50\\n16, 2048375.31153.95158.7745.972.442.368.16\\n16, 3840243.66OOM OOM28.33 8.60\\nBatch\\nSize,\\nSequence\\nLengthONNX\\nRT\\nFP16PyTorch\\nEager\\nFP16PyTorch\\nCompile\\nFP16Llama.cppFP16\\nSpeedUp\\nORT/PyTorch\\nEagerFP16\\nSpeedUp\\nORT/PyTorch\\nCompileFP16 SpeedUp\\nORT/Llama.cpp\\n1, 16137.3026.0226.83125.865.285.121.09'},\n",
       " {'page_number': 4,\n",
       "  'text': 'model_card.md 2024-10-07\\n4 / 5Batch\\nSize,\\nSequence\\nLengthONNX\\nRT\\nFP16PyTorch\\nEager\\nFP16PyTorch\\nCompile\\nFP16Llama.cppFP16\\nSpeedUp\\nORT/PyTorch\\nEagerFP16\\nSpeedUp\\nORT/PyTorch\\nCompileFP16 SpeedUp\\nORT/Llama.cpp\\n1, 64135.7926.0126.48125.755.225.131.08\\n1, 256127.9226.1726.61125.244.894.811.02\\n1, 1024114.0826.1126.63117.974.374.280.97\\n1, 2048101.6817.7721.05111.085.724.830.92\\n1, 384084.9425.1726.77104.883.373.170.81\\n4, 16529.0799.47100.22124.635.325.284.25\\n4, 64513.8599.47100.54123.205.175.114.17\\n4, 256466.5699.21100.22117.614.704.663.97\\n4, 1024352.0699.56100.50100.423.543.503.51\\n4, 2048271.0270.1273.6683.953.863.683.23\\n4, 3840191.3674.3579.6865.512.572.402.92\\n8, 16936.46198.99212.40120.244.714.417.79\\n8, 64926.83200.28213.97117.774.634.337.87\\n8, 256783.95200.66214.88108.333.913.657.24\\n8, 1024511.96183.10201.0182.522.802.556.20\\n8, 2048352.8696.99122.1062.413.642.895.65\\n8, 3840228.9796.81101.6043.892.372.255.22\\n16, 161675.72396.52422.13112.784.233.9714.86\\n16, 641591.61395.66422.47108.364.023.7714.69\\n16, 2561249.94399.30429.1093.683.132.9113.34\\n16, 1024685.63270.99292.2460.662.532.3511.30\\n16, 2048441.15121.17162.9341.303.642.7110.68\\n16, 3840270.38OOM OOM26.500.000.0010.20\\nThe table below shows the average throughput of the first 256 tokens generated (tps) for INT4 precision on CPU\\nas measured on a Standard D16s v6 (16 vcpus, 64 GiB memory)\\nBatch Size, Sequence LengthORT INT4 AWQLlama.cpp INT4INT4 AWQ SpeedUp Llama.cpp\\n1, 16 41.9926.721.57\\n1, 64 41.8126.671.57\\n1, 256 41.2626.301.57'},\n",
       " {'page_number': 5,\n",
       "  'text': 'model_card.md 2024-10-07\\n5 / 5Batch Size, Sequence LengthORT INT4 AWQLlama.cpp INT4INT4 AWQ SpeedUp Llama.cpp\\n1, 1024 37.1524.021.55\\n1, 2048 32.6821.821.50\\nPackage Versions\\nPip package nameVersion\\ntorch 2.4.1\\ntriton 3.0.0\\nonnxruntime-gpu1.19.2\\nonnxruntime-genai0.4.0\\nonnxruntime-genai-cuda0.4.0\\ntransformers4.44.2\\nllama.cpp bdf314f38a2c90e18285f7d7067e8d736a14000a\\nAppendix\\nActivation Aware Quantization (AWQ) works by identifying the top 1% most salient weights that are most important\\nfor maintaining accuracy and quantizing the remaining 99% of weights. This leads to less accuracy loss from\\nquantization compared to many other quantization techniques. For more on AWQ, see here.\\nModel Card Contact\\nparinitarahi\\nContributors\\nSunghoon Choi, Yufeng Li, Kunal Vaishnavi, Akshay Sonawane, Rui Ren, Parinita Rahi\\nLicense\\nThe model is licensed under the MIT license.\\nTrademarks\\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\\ntrademarks or logos is subject to and must follow\\u202fMicrosoftʼs Trademark & Brand Guidelines. Use of Microsoft\\ntrademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\\nAny use of third-party trademarks or logos are subject to those third-partyʼs policies.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ''\n",
    "\n",
    "for item in pages_text:\n",
    "    content += item['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_card.md 2024-10-07\\n1 / 5Phi-3.5-Mini-Instruct ONNX models\\nThis repository hosts the optimized versions of Phi-3.5-mini-instruct to accelerate inference with ONNX Runtime.\\nOptimized Phi-3.5 Mini models are published here in ONNX format to run with ONNX Runtime on CPU and GPU\\nacross devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the\\nprecision best suited to each of these targets.\\nTo easily get started with Phi-3.5, you can use our newly introduced ONNX Runtime Generate() API. See here for\\ninstructions on how to run it.\\nONNX Models\\nHere are some of the optimized configurations we have added:\\n\\x00. ONNX model for fp16 CUDA: ONNX model you can use to run for your NVIDIA GPUs.\\n\\x00. ONNX model for int4 CUDA: ONNX model for NVIDIA GPUs using int4 quantization via AWQ.\\n\\x00. ONNX model for int4 CPU and Mobile: ONNX model for CPU and mobile using int4 quantization via AWQ.\\nModel Summary\\nPhi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and\\nfiltered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to\\nthe Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement\\nprocess, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference\\noptimization to ensure precise instruction adherence and robust safety measures.\\nIntended Uses\\nThe Phi 3.5 model is intended for commercial and research use in multiple languages. The model provides uses for\\ngeneral purpose AI systems and applications which require:\\n\\x00. Memory/compute constrained environments\\n\\x00. Latency bound scenarios\\n\\x00. Strong reasoning (especially code, math and logic)\\nUse Case Considerations\\nPhi 3.5 models are not specifically designed or evaluated for all downstream purposes. Developers should\\nconsider common limitations of language models as they select use cases, and evaluate and mitigate for\\naccuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk\\nscenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade\\ncompliance laws, etc.) that are relevant to their use case. Nothing contained in this Model Card should be\\ninterpreted as or deemed a restriction or modification to the license the model is released under.\\nRelease Notes\\nThis is an update over the instruction-tuned Phi-3 Mini ONNX model release. We believe most use cases will\\nbenefit from this release, but we encourage users to test their particular AI applications. We appreciate the\\nenthusiastic adoption of the Phi-3 model family and continue to welcome all feedback from the community.\\nHardware Supportedmodel_card.md 2024-10-07\\n2 / 5The ONNX models are tested on:\\nGPU SKU: RTX 4090 (DirectML)\\nGPU SKU: 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4 (CUDA)\\nCPU SKU: Standard D16s v6 (16 vcpus, 64 GiB memory)\\nAMD CPU: Internal_D64as_v5\\nMinimum Configuration Required:\\nWindows: DirectX 12-capable GPU and a minimum of 4GB of combined RAM\\nCUDA: NVIDIA GPU with Compute Capability >= 7.0\\nModel Description\\nDeveloped by: Microsoft\\nModel type: ONNX\\nLanguage(s) (NLP): Python, C, C++\\nLicense: MIT\\nModel Description: This is a conversion of the Phi-3.5 Mini-Instruct model for ONNX Runtime inference.\\nHow to Get Started with the Model\\nTo make running of the Phi-3 models across a range of devices and platforms across various execution provider\\nbackends possible, we introduce a new API to wrap several aspects of generative AI inferencing. This API make it\\neasy to drag and drop LLMs straight into your app. For running the early version of these models with ONNX\\nRuntime, follow the steps here.\\nFor example:\\npython model-qa.py -m /*{YourModelPath}*/Phi -3.5-mini-instruct-\\nonnx/cpu_and_mobile/cpu-int4-awq-block -128-acc-level -4 -k 40 -p 0.95 -t 0.8 -r \\n1.0\\n*Input:*  <|user|>Tell me a joke<|end|><|assistant|>  \\n \\n*Output:*  Why don\\'t scientists trust atoms?  \\n           Because they make up everything!  \\n \\nThis joke plays on the double meaning of \"make up.\" In science, atoms are the  \\nfundamental building blocks of matter, literally making up everything. However,  \\nin a colloquial sense, \"to make up\" can mean to fabricate or lie, hence the  \\nhumor. \\nPerformance Metrics\\nPhi-3.5 Mini-Instruct performs better in ONNX Runtime than PyTorch for all batch size, prompt length\\ncombinations.\\nThe table below shows the average throughput of the first 256 tokens generated (tps) for FP16 and INT4\\nprecisions on CUDA as measured on 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4. ONNX Runtimemodel_card.md 2024-10-07\\n3 / 5models for GPU are 21X faster than PyTorch Compile and up to 8X faster than llama.cpp on A100 GPU.\\nBatch\\nSize,\\nSequence\\nLengthONNX\\nRT\\nINT4PyTorch\\nEager\\nINT4PyTorch\\nCompile\\nINT4Llama.cpp\\nINT4INT4\\nSpeedUp\\nORT/PyTorch\\nEagerINT4\\nSpeedUp\\nORT/PyTorch\\nCompileINT4 SpeedUp\\nORT/Llama.cpp\\n1, 16238.9717.7511.36183.1713.4621.041.30\\n1, 64233.7417.7411.32182.7713.1720.651.28\\n1, 256208.5217.8211.34182.1511.7018.381.14\\n1, 1024174.1917.8511.36166.399.7615.341.05\\n1, 2048146.1017.9611.35153.508.1412.870.95\\n1, 3840112.6817.9111.34141.536.299.940.80\\n4, 16286.7360.9040.89180.824.717.011.59\\n4, 64282.8760.8841.03177.694.656.891.59\\n4, 256268.3060.8540.90166.344.416.561.61\\n4, 1024223.3060.8640.90133.393.675.461.67\\n4, 2048187.6260.8040.93106.033.094.581.77\\n4, 3840145.5955.9640.8878.122.603.561.86\\n8, 16541.04121.9281.96171.904.446.603.15\\n8, 64532.68121.8781.98166.334.376.503.20\\n8, 256480.00122.0681.80148.073.935.873.24\\n8, 1024360.60122.4881.59103.582.944.423.48\\n8, 2048274.16105.9281.7174.012.593.363.70\\n8, 3840192.5079.7481.5049.232.412.363.91\\n16, 161007.69244.16163.09156.994.136.186.42\\n16, 64966.42244.89163.26148.233.955.926.52\\n16, 256827.37244.84163.23121.853.385.076.79\\n16, 1024536.73209.13169.3071.572.573.177.50\\n16, 2048375.31153.95158.7745.972.442.368.16\\n16, 3840243.66OOM OOM28.33 8.60\\nBatch\\nSize,\\nSequence\\nLengthONNX\\nRT\\nFP16PyTorch\\nEager\\nFP16PyTorch\\nCompile\\nFP16Llama.cppFP16\\nSpeedUp\\nORT/PyTorch\\nEagerFP16\\nSpeedUp\\nORT/PyTorch\\nCompileFP16 SpeedUp\\nORT/Llama.cpp\\n1, 16137.3026.0226.83125.865.285.121.09model_card.md 2024-10-07\\n4 / 5Batch\\nSize,\\nSequence\\nLengthONNX\\nRT\\nFP16PyTorch\\nEager\\nFP16PyTorch\\nCompile\\nFP16Llama.cppFP16\\nSpeedUp\\nORT/PyTorch\\nEagerFP16\\nSpeedUp\\nORT/PyTorch\\nCompileFP16 SpeedUp\\nORT/Llama.cpp\\n1, 64135.7926.0126.48125.755.225.131.08\\n1, 256127.9226.1726.61125.244.894.811.02\\n1, 1024114.0826.1126.63117.974.374.280.97\\n1, 2048101.6817.7721.05111.085.724.830.92\\n1, 384084.9425.1726.77104.883.373.170.81\\n4, 16529.0799.47100.22124.635.325.284.25\\n4, 64513.8599.47100.54123.205.175.114.17\\n4, 256466.5699.21100.22117.614.704.663.97\\n4, 1024352.0699.56100.50100.423.543.503.51\\n4, 2048271.0270.1273.6683.953.863.683.23\\n4, 3840191.3674.3579.6865.512.572.402.92\\n8, 16936.46198.99212.40120.244.714.417.79\\n8, 64926.83200.28213.97117.774.634.337.87\\n8, 256783.95200.66214.88108.333.913.657.24\\n8, 1024511.96183.10201.0182.522.802.556.20\\n8, 2048352.8696.99122.1062.413.642.895.65\\n8, 3840228.9796.81101.6043.892.372.255.22\\n16, 161675.72396.52422.13112.784.233.9714.86\\n16, 641591.61395.66422.47108.364.023.7714.69\\n16, 2561249.94399.30429.1093.683.132.9113.34\\n16, 1024685.63270.99292.2460.662.532.3511.30\\n16, 2048441.15121.17162.9341.303.642.7110.68\\n16, 3840270.38OOM OOM26.500.000.0010.20\\nThe table below shows the average throughput of the first 256 tokens generated (tps) for INT4 precision on CPU\\nas measured on a Standard D16s v6 (16 vcpus, 64 GiB memory)\\nBatch Size, Sequence LengthORT INT4 AWQLlama.cpp INT4INT4 AWQ SpeedUp Llama.cpp\\n1, 16 41.9926.721.57\\n1, 64 41.8126.671.57\\n1, 256 41.2626.301.57model_card.md 2024-10-07\\n5 / 5Batch Size, Sequence LengthORT INT4 AWQLlama.cpp INT4INT4 AWQ SpeedUp Llama.cpp\\n1, 1024 37.1524.021.55\\n1, 2048 32.6821.821.50\\nPackage Versions\\nPip package nameVersion\\ntorch 2.4.1\\ntriton 3.0.0\\nonnxruntime-gpu1.19.2\\nonnxruntime-genai0.4.0\\nonnxruntime-genai-cuda0.4.0\\ntransformers4.44.2\\nllama.cpp bdf314f38a2c90e18285f7d7067e8d736a14000a\\nAppendix\\nActivation Aware Quantization (AWQ) works by identifying the top 1% most salient weights that are most important\\nfor maintaining accuracy and quantizing the remaining 99% of weights. This leads to less accuracy loss from\\nquantization compared to many other quantization techniques. For more on AWQ, see here.\\nModel Card Contact\\nparinitarahi\\nContributors\\nSunghoon Choi, Yufeng Li, Kunal Vaishnavi, Akshay Sonawane, Rui Ren, Parinita Rahi\\nLicense\\nThe model is licensed under the MIT license.\\nTrademarks\\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\\ntrademarks or logos is subject to and must follow\\u202fMicrosoftʼs Trademark & Brand Guidelines. Use of Microsoft\\ntrademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\\nAny use of third-party trademarks or logos are subject to those third-partyʼs policies.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Phi-3.5-mini-instruct\"\n",
    "token =  'Your GitHub Model token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_card.md 2024-10-07\\n1 / 5Phi-3.5-Mini-Instruct ONNX models\\nThis repository hosts the optimized versions of Phi-3.5-mini-instruct to accelerate inference with ONNX Runtime.\\nOptimized Phi-3.5 Mini models are published here in ONNX format to run with ONNX Runtime on CPU and GPU\\nacross devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the\\nprecision best suited to each of these targets.\\nTo easily get started with Phi-3.5, you can use our newly introduced ONNX Runtime Generate() API. See here for\\ninstructions on how to run it.\\nONNX Models\\nHere are some of the optimized configurations we have added:\\n\\x00. ONNX model for fp16 CUDA: ONNX model you can use to run for your NVIDIA GPUs.\\n\\x00. ONNX model for int4 CUDA: ONNX model for NVIDIA GPUs using int4 quantization via AWQ.\\n\\x00. ONNX model for int4 CPU and Mobile: ONNX model for CPU and mobile using int4 quantization via AWQ.\\nModel Summary\\nPhi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and\\nfiltered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to\\nthe Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement\\nprocess, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference\\noptimization to ensure precise instruction adherence and robust safety measures.\\nIntended Uses\\nThe Phi 3.5 model is intended for commercial and research use in multiple languages. The model provides uses for\\ngeneral purpose AI systems and applications which require:\\n\\x00. Memory/compute constrained environments\\n\\x00. Latency bound scenarios\\n\\x00. Strong reasoning (especially code, math and logic)\\nUse Case Considerations\\nPhi 3.5 models are not specifically designed or evaluated for all downstream purposes. Developers should\\nconsider common limitations of language models as they select use cases, and evaluate and mitigate for\\naccuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk\\nscenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade\\ncompliance laws, etc.) that are relevant to their use case. Nothing contained in this Model Card should be\\ninterpreted as or deemed a restriction or modification to the license the model is released under.\\nRelease Notes\\nThis is an update over the instruction-tuned Phi-3 Mini ONNX model release. We believe most use cases will\\nbenefit from this release, but we encourage users to test their particular AI applications. We appreciate the\\nenthusiastic adoption of the Phi-3 model family and continue to welcome all feedback from the community.\\nHardware Supportedmodel_card.md 2024-10-07\\n2 / 5The ONNX models are tested on:\\nGPU SKU: RTX 4090 (DirectML)\\nGPU SKU: 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4 (CUDA)\\nCPU SKU: Standard D16s v6 (16 vcpus, 64 GiB memory)\\nAMD CPU: Internal_D64as_v5\\nMinimum Configuration Required:\\nWindows: DirectX 12-capable GPU and a minimum of 4GB of combined RAM\\nCUDA: NVIDIA GPU with Compute Capability >= 7.0\\nModel Description\\nDeveloped by: Microsoft\\nModel type: ONNX\\nLanguage(s) (NLP): Python, C, C++\\nLicense: MIT\\nModel Description: This is a conversion of the Phi-3.5 Mini-Instruct model for ONNX Runtime inference.\\nHow to Get Started with the Model\\nTo make running of the Phi-3 models across a range of devices and platforms across various execution provider\\nbackends possible, we introduce a new API to wrap several aspects of generative AI inferencing. This API make it\\neasy to drag and drop LLMs straight into your app. For running the early version of these models with ONNX\\nRuntime, follow the steps here.\\nFor example:\\npython model-qa.py -m /*{YourModelPath}*/Phi -3.5-mini-instruct-\\nonnx/cpu_and_mobile/cpu-int4-awq-block -128-acc-level -4 -k 40 -p 0.95 -t 0.8 -r \\n1.0\\n*Input:*  <|user|>Tell me a joke<|end|><|assistant|>  \\n \\n*Output:*  Why don\\'t scientists trust atoms?  \\n           Because they make up everything!  \\n \\nThis joke plays on the double meaning of \"make up.\" In science, atoms are the  \\nfundamental building blocks of matter, literally making up everything. However,  \\nin a colloquial sense, \"to make up\" can mean to fabricate or lie, hence the  \\nhumor. \\nPerformance Metrics\\nPhi-3.5 Mini-Instruct performs better in ONNX Runtime than PyTorch for all batch size, prompt length\\ncombinations.\\nThe table below shows the average throughput of the first 256 tokens generated (tps) for FP16 and INT4\\nprecisions on CUDA as measured on 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4. ONNX Runtimemodel_card.md 2024-10-07\\n3 / 5models for GPU are 21X faster than PyTorch Compile and up to 8X faster than llama.cpp on A100 GPU.\\nBatch\\nSize,\\nSequence\\nLengthONNX\\nRT\\nINT4PyTorch\\nEager\\nINT4PyTorch\\nCompile\\nINT4Llama.cpp\\nINT4INT4\\nSpeedUp\\nORT/PyTorch\\nEagerINT4\\nSpeedUp\\nORT/PyTorch\\nCompileINT4 SpeedUp\\nORT/Llama.cpp\\n1, 16238.9717.7511.36183.1713.4621.041.30\\n1, 64233.7417.7411.32182.7713.1720.651.28\\n1, 256208.5217.8211.34182.1511.7018.381.14\\n1, 1024174.1917.8511.36166.399.7615.341.05\\n1, 2048146.1017.9611.35153.508.1412.870.95\\n1, 3840112.6817.9111.34141.536.299.940.80\\n4, 16286.7360.9040.89180.824.717.011.59\\n4, 64282.8760.8841.03177.694.656.891.59\\n4, 256268.3060.8540.90166.344.416.561.61\\n4, 1024223.3060.8640.90133.393.675.461.67\\n4, 2048187.6260.8040.93106.033.094.581.77\\n4, 3840145.5955.9640.8878.122.603.561.86\\n8, 16541.04121.9281.96171.904.446.603.15\\n8, 64532.68121.8781.98166.334.376.503.20\\n8, 256480.00122.0681.80148.073.935.873.24\\n8, 1024360.60122.4881.59103.582.944.423.48\\n8, 2048274.16105.9281.7174.012.593.363.70\\n8, 3840192.5079.7481.5049.232.412.363.91\\n16, 161007.69244.16163.09156.994.136.186.42\\n16, 64966.42244.89163.26148.233.955.926.52\\n16, 256827.37244.84163.23121.853.385.076.79\\n16, 1024536.73209.13169.3071.572.573.177.50\\n16, 2048375.31153.95158.7745.972.442.368.16\\n16, 3840243.66OOM OOM28.33 8.60\\nBatch\\nSize,\\nSequence\\nLengthONNX\\nRT\\nFP16PyTorch\\nEager\\nFP16PyTorch\\nCompile\\nFP16Llama.cppFP16\\nSpeedUp\\nORT/PyTorch\\nEagerFP16\\nSpeedUp\\nORT/PyTorch\\nCompileFP16 SpeedUp\\nORT/Llama.cpp\\n1, 16137.3026.0226.83125.865.285.121.09model_card.md 2024-10-07\\n4 / 5Batch\\nSize,\\nSequence\\nLengthONNX\\nRT\\nFP16PyTorch\\nEager\\nFP16PyTorch\\nCompile\\nFP16Llama.cppFP16\\nSpeedUp\\nORT/PyTorch\\nEagerFP16\\nSpeedUp\\nORT/PyTorch\\nCompileFP16 SpeedUp\\nORT/Llama.cpp\\n1, 64135.7926.0126.48125.755.225.131.08\\n1, 256127.9226.1726.61125.244.894.811.02\\n1, 1024114.0826.1126.63117.974.374.280.97\\n1, 2048101.6817.7721.05111.085.724.830.92\\n1, 384084.9425.1726.77104.883.373.170.81\\n4, 16529.0799.47100.22124.635.325.284.25\\n4, 64513.8599.47100.54123.205.175.114.17\\n4, 256466.5699.21100.22117.614.704.663.97\\n4, 1024352.0699.56100.50100.423.543.503.51\\n4, 2048271.0270.1273.6683.953.863.683.23\\n4, 3840191.3674.3579.6865.512.572.402.92\\n8, 16936.46198.99212.40120.244.714.417.79\\n8, 64926.83200.28213.97117.774.634.337.87\\n8, 256783.95200.66214.88108.333.913.657.24\\n8, 1024511.96183.10201.0182.522.802.556.20\\n8, 2048352.8696.99122.1062.413.642.895.65\\n8, 3840228.9796.81101.6043.892.372.255.22\\n16, 161675.72396.52422.13112.784.233.9714.86\\n16, 641591.61395.66422.47108.364.023.7714.69\\n16, 2561249.94399.30429.1093.683.132.9113.34\\n16, 1024685.63270.99292.2460.662.532.3511.30\\n16, 2048441.15121.17162.9341.303.642.7110.68\\n16, 3840270.38OOM OOM26.500.000.0010.20\\nThe table below shows the average throughput of the first 256 tokens generated (tps) for INT4 precision on CPU\\nas measured on a Standard D16s v6 (16 vcpus, 64 GiB memory)\\nBatch Size, Sequence LengthORT INT4 AWQLlama.cpp INT4INT4 AWQ SpeedUp Llama.cpp\\n1, 16 41.9926.721.57\\n1, 64 41.8126.671.57\\n1, 256 41.2626.301.57model_card.md 2024-10-07\\n5 / 5Batch Size, Sequence LengthORT INT4 AWQLlama.cpp INT4INT4 AWQ SpeedUp Llama.cpp\\n1, 1024 37.1524.021.55\\n1, 2048 32.6821.821.50\\nPackage Versions\\nPip package nameVersion\\ntorch 2.4.1\\ntriton 3.0.0\\nonnxruntime-gpu1.19.2\\nonnxruntime-genai0.4.0\\nonnxruntime-genai-cuda0.4.0\\ntransformers4.44.2\\nllama.cpp bdf314f38a2c90e18285f7d7067e8d736a14000a\\nAppendix\\nActivation Aware Quantization (AWQ) works by identifying the top 1% most salient weights that are most important\\nfor maintaining accuracy and quantizing the remaining 99% of weights. This leads to less accuracy loss from\\nquantization compared to many other quantization techniques. For more on AWQ, see here.\\nModel Card Contact\\nparinitarahi\\nContributors\\nSunghoon Choi, Yufeng Li, Kunal Vaishnavi, Akshay Sonawane, Rui Ren, Parinita Rahi\\nLicense\\nThe model is licensed under the MIT license.\\nTrademarks\\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\\ntrademarks or logos is subject to and must follow\\u202fMicrosoftʼs Trademark & Brand Guidelines. Use of Microsoft\\ntrademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\\nAny use of third-party trademarks or logos are subject to those third-partyʼs policies.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"\"\"You are an expert in content chunking. Please help me chunk user's input text according to the following requirements\n",
    "1. Truncate the text content into chunks of no more than 300 tokens. \n",
    "2. Each chunk part should maintain contextual coherence. The truncated content should be retained in its entirety without any additions or modifications.\n",
    "2. Each chunked part is output as JSON {\"chunking\":\"\"}\n",
    "3. The final result is a JSON array [{\"chunking\":\"\"},{\"chunking\":\"\"},{\"chunking\":\"\"}....]\"\"\"),\n",
    "        UserMessage(content=content),\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    top_p=1.0,\n",
    "    max_tokens=100000,\n",
    "    model=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [\\n  {\"chunking\": \"This repository hosts the optimized versions of Phi-3.5-mini-instruct to accelerate inference with ONNX Runtime. Optimized Phi-3 Mini models are published here in ONNX format to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets.\"},\\n  {\"chunking\": \"To easily get started with Phi-3.5, you can use our newly introduced ONNX Runtime Generate() API. See here for instructions on how to run it.\"},\\n  {\"chunking\": \"Here are some of the optimized configurations we have added: ONNX model for fp16 CUDA: ONNX model for NVIDIA GPUs. ONNX model for int4 CUDA: ONNX model for NVIDIA GPUs using int4 quantization via AWQ. ONNX model for int4 CPU and Mobile: ONNX model for CPU and mobile using int4 quantization via AWQ.\"},\\n  {\"chunking\": \"Model Summary: Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\"},\\n  {\"chunking\": \"Intended Uses: The Phi 3.5 model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require: Memory/compute constrained environments, Latency bound scenarios, Strong reasoning (especially code, math and logic). Use Case Considerations: Phi 3.5 models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios.\"},\\n  {\"chunking\": \"Release Notes: This is an update over the instruction-tuned Phi-3 Mini ONNX model release. We believe most use cases will benefit from this release, but we encourage users to test their particular AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family and continue to welcome all feedback from the community.\"},\\n  {\"chunking\": \"Hardware Supported: GPU SKU: RTX 4090 (DirectML) GPU SKU: 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4. CPU SKU: Standard D16s v6 (16 vcpus, 64 GiB memory). Minimum Configuration Required: Windows: DirectX 12-capable GPU and a minimum of 4GB of combined RAM CUDA: NVIDIA GPU with Compute Capability >= 7.0 Model Description: This is a conversion of the Phi-3.5 Mini-Instruct model for ONNX Runtime inference. How to Get Started with the Model: To make running of the Phi-3 models across a range of devices and platforms across various execution provider backends possible, we introduce a new API to wrap several aspects of generative AI inferencing. This API make it easy to drag and drop LLMs straight into your app. For running the early version of these models with ONNX Runtime, follow the steps here.\"},\\n  {\"chunking\": \"For example: python model-qa.py -m /*{YourModelPath}*/Phi -3.5-mini-instruct-onnx/cpu_and_mobile/cpu-int4-awq-block -128-acc-level -4 -k 40 -p 0.95 -t 0.8 -r 1.0 Input: python model-qa.py -m /*{YourModelPath}*/Phi -3.5-mini-instruct-onnx/cpu_and_mobile/cpu-int4-awq-block -128-acc-level -4 -k 40 -p 0.95 -t 0.8 -r 1.0\"},\\n  {\"chunking\": \"Batch Size, Sequence LengthORT INT4 AWQLlama.cpp INT4INT4 AWQ SpeedUp Llama.cpp 1, 16 41.9926.721.57 1, 64 41.8126.671.57 1, 256 41.2626.301.57 1, 1024 37.1524.021.55 1, 2048 32.6821.821.50 Package Versions: Pip package nameVersion torch 2.4.1 triton 3.0.0 onnxruntime-gpu1.19.2 onnxruntime-genai0.4.0 onnxruntime-genai-cuda0.4.0 transformers4.44.2 llama.cpp bdf314f38a2c90e18285f7d7067e8d736a14000a Model Card Contact: parinitarahi Contributors: Sunghoon Choi, Yufeng Li, Kunal Vaishnavi, Akshay Sonawane, Rui Ren, Parinita Rahi License: The model is licensed under the MIT license. Trademarks: This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft\\'s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\\'s policies.\"}\\n]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunking': 'This repository hosts the optimized versions of Phi-3.5-mini-instruct to accelerate inference with ONNX Runtime. Optimized Phi-3 Mini models are published here in ONNX format to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets.'},\n",
       " {'chunking': 'To easily get started with Phi-3.5, you can use our newly introduced ONNX Runtime Generate() API. See here for instructions on how to run it.'},\n",
       " {'chunking': 'Here are some of the optimized configurations we have added: ONNX model for fp16 CUDA: ONNX model for NVIDIA GPUs. ONNX model for int4 CUDA: ONNX model for NVIDIA GPUs using int4 quantization via AWQ. ONNX model for int4 CPU and Mobile: ONNX model for CPU and mobile using int4 quantization via AWQ.'},\n",
       " {'chunking': 'Model Summary: Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.'},\n",
       " {'chunking': 'Intended Uses: The Phi 3.5 model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require: Memory/compute constrained environments, Latency bound scenarios, Strong reasoning (especially code, math and logic). Use Case Considerations: Phi 3.5 models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios.'},\n",
       " {'chunking': 'Release Notes: This is an update over the instruction-tuned Phi-3 Mini ONNX model release. We believe most use cases will benefit from this release, but we encourage users to test their particular AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family and continue to welcome all feedback from the community.'},\n",
       " {'chunking': 'Hardware Supported: GPU SKU: RTX 4090 (DirectML) GPU SKU: 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4. CPU SKU: Standard D16s v6 (16 vcpus, 64 GiB memory). Minimum Configuration Required: Windows: DirectX 12-capable GPU and a minimum of 4GB of combined RAM CUDA: NVIDIA GPU with Compute Capability >= 7.0 Model Description: This is a conversion of the Phi-3.5 Mini-Instruct model for ONNX Runtime inference. How to Get Started with the Model: To make running of the Phi-3 models across a range of devices and platforms across various execution provider backends possible, we introduce a new API to wrap several aspects of generative AI inferencing. This API make it easy to drag and drop LLMs straight into your app. For running the early version of these models with ONNX Runtime, follow the steps here.'},\n",
       " {'chunking': 'For example: python model-qa.py -m /*{YourModelPath}*/Phi -3.5-mini-instruct-onnx/cpu_and_mobile/cpu-int4-awq-block -128-acc-level -4 -k 40 -p 0.95 -t 0.8 -r 1.0 Input: python model-qa.py -m /*{YourModelPath}*/Phi -3.5-mini-instruct-onnx/cpu_and_mobile/cpu-int4-awq-block -128-acc-level -4 -k 40 -p 0.95 -t 0.8 -r 1.0'},\n",
       " {'chunking': \"Batch Size, Sequence LengthORT INT4 AWQLlama.cpp INT4INT4 AWQ SpeedUp Llama.cpp 1, 16 41.9926.721.57 1, 64 41.8126.671.57 1, 256 41.2626.301.57 1, 1024 37.1524.021.55 1, 2048 32.6821.821.50 Package Versions: Pip package nameVersion torch 2.4.1 triton 3.0.0 onnxruntime-gpu1.19.2 onnxruntime-genai0.4.0 onnxruntime-genai-cuda0.4.0 transformers4.44.2 llama.cpp bdf314f38a2c90e18285f7d7067e8d736a14000a Model Card Contact: parinitarahi Contributors: Sunghoon Choi, Yufeng Li, Kunal Vaishnavi, Akshay Sonawane, Rui Ren, Parinita Rahi License: The model is licensed under the MIT license. Trademarks: This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\"}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
